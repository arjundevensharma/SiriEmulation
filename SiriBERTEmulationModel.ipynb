{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAggs6NkOOK/NqvGh7W9qt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjundevensharma/SiriEmulation/blob/main/SiriBERTEmulationModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4DWC7NzjCkO",
        "outputId": "6d840c6b-2115-440b-d503-20d3259e29e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "--2024-03-28 10:43:33--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/train\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.196.207, 173.194.218.207, 108.177.11.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.196.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1793794 (1.7M) [application/octet-stream]\n",
            "Saving to: ‘train.1’\n",
            "\n",
            "train.1             100%[===================>]   1.71M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-03-28 10:43:34 (87.5 MB/s) - ‘train.1’ saved [1793794/1793794]\n",
            "\n",
            "--2024-03-28 10:43:34--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/valid\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.196.207, 173.194.218.207, 108.177.11.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.196.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 98609 (96K) [application/octet-stream]\n",
            "Saving to: ‘valid.1’\n",
            "\n",
            "valid.1             100%[===================>]  96.30K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-03-28 10:43:34 (81.5 MB/s) - ‘valid.1’ saved [98609/98609]\n",
            "\n",
            "--2024-03-28 10:43:34--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/test\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.196.207, 173.194.218.207, 108.177.11.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.196.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96761 (94K) [application/octet-stream]\n",
            "Saving to: ‘test.1’\n",
            "\n",
            "test.1              100%[===================>]  94.49K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-03-28 10:43:34 (74.9 MB/s) - ‘test.1’ saved [96761/96761]\n",
            "\n",
            "--2024-03-28 10:43:34--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/vocab.intent\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.196.207, 173.194.218.207, 108.177.11.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.196.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99 [application/octet-stream]\n",
            "Saving to: ‘vocab.intent.1’\n",
            "\n",
            "vocab.intent.1      100%[===================>]      99  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-28 10:43:34 (59.1 MB/s) - ‘vocab.intent.1’ saved [99/99]\n",
            "\n",
            "--2024-03-28 10:43:34--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/vocab.slot\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.196.207, 173.194.218.207, 108.177.11.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.196.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 994 [application/octet-stream]\n",
            "Saving to: ‘vocab.slot.1’\n",
            "\n",
            "vocab.slot.1        100%[===================>]     994  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-28 10:43:34 (12.3 MB/s) - ‘vocab.slot.1’ saved [994/994]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "409/409 [==============================] - 8403s 21s/step - loss: 0.5053 - output_1_loss: 0.2775 - output_2_loss: 0.2278 - output_1_accuracy: 0.9374 - output_2_accuracy: 0.9239 - val_loss: 0.1290 - val_output_1_loss: 0.0495 - val_output_2_loss: 0.0795 - val_output_1_accuracy: 0.9863 - val_output_2_accuracy: 0.9814\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intent': 'BookRestaurant',\n",
              " 'slots': {'party_size_number': 'two',\n",
              "  'restaurant_name': 'Le Ritz',\n",
              "  'timeRange': 'Friday'}}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "%pip install -q transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "from urllib.request import urlretrieve\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.layers import Dropout, Dense\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/train'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/valid'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/test'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/vocab.intent'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/vocab.slot'\n",
        "\n",
        "\n",
        "def parse_line(line):\n",
        "    data, intent_label = line.split(\" <=> \")\n",
        "    items = data.split()\n",
        "    words = [item.rsplit(\":\", 1)[0]for item in items]\n",
        "    word_labels = [item.rsplit(\":\", 1)[1]for item in items]\n",
        "    return {\n",
        "        \"intent_label\": intent_label,\n",
        "        \"words\": \" \".join(words),\n",
        "        \"word_labels\": \" \".join(word_labels),\n",
        "        \"length\": len(words),\n",
        "    }\n",
        "\n",
        "def encode_dataset(text_sequences):\n",
        "    token_ids = np.zeros(shape=(len(text_sequences), max_token_len),\n",
        "                         dtype=np.int32)\n",
        "\n",
        "    for i, text_sequence in enumerate(text_sequences):\n",
        "        encoded = tokenizer.encode(text_sequence)\n",
        "        token_ids[i, 0:len(encoded)] = encoded\n",
        "\n",
        "    attention_masks = (token_ids != 0).astype(np.int32)\n",
        "    return {\"input_ids\": token_ids, \"attention_masks\": attention_masks}\n",
        "\n",
        "\n",
        "train_lines = Path(\"train\").read_text().strip().splitlines()\n",
        "valid_lines = Path(\"valid\").read_text().strip().splitlines()\n",
        "test_lines = Path(\"test\").read_text().strip().splitlines()\n",
        "\n",
        "df_train = pd.DataFrame([parse_line(line) for line in train_lines])\n",
        "df_valid = pd.DataFrame([parse_line(line) for line in valid_lines])\n",
        "df_test = pd.DataFrame([parse_line(line) for line in test_lines])\n",
        "\n",
        "max_token_len = 43\n",
        "\n",
        "encoded_train = encode_dataset(df_train[\"words\"])\n",
        "encoded_valid = encode_dataset(df_valid[\"words\"])\n",
        "encoded_test = encode_dataset(df_test[\"words\"])\n",
        "\n",
        "intent_names = Path(\"vocab.intent\").read_text().split()\n",
        "intent_map = dict((label, idx) for idx, label in enumerate(intent_names))\n",
        "intent_train = df_train[\"intent_label\"].map(intent_map).values\n",
        "intent_valid = df_valid[\"intent_label\"].map(intent_map).values\n",
        "intent_test = df_test[\"intent_label\"].map(intent_map).values\n",
        "\n",
        "base_bert_model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "slot_names = [\"[PAD]\"] + Path(\"vocab.slot\").read_text().strip().splitlines()\n",
        "slot_map = {}\n",
        "for label in slot_names:\n",
        "    slot_map[label] = len(slot_map)\n",
        "\n",
        "def encode_token_labels(text_sequences, true_word_labels):\n",
        "    encoded = np.zeros(shape=(len(text_sequences), max_token_len), dtype=np.int32)\n",
        "    for i, (text_sequence, word_labels) in enumerate( \\\n",
        "            zip(text_sequences, true_word_labels)):\n",
        "        encoded_labels = []\n",
        "        for word, word_label in zip(text_sequence.split(), word_labels.split()):\n",
        "            tokens = tokenizer.tokenize(word)\n",
        "            encoded_labels.append(slot_map[word_label])\n",
        "            expand_label = word_label.replace(\"B-\", \"I-\")\n",
        "            if not expand_label in slot_map:\n",
        "                expand_label = word_label\n",
        "            encoded_labels.extend([slot_map[expand_label]] * (len(tokens) - 1))\n",
        "        encoded[i, 1:len(encoded_labels) + 1] = encoded_labels\n",
        "    return encoded\n",
        "\n",
        "slot_train = encode_token_labels(df_train[\"words\"], df_train[\"word_labels\"])\n",
        "slot_valid = encode_token_labels(df_valid[\"words\"], df_valid[\"word_labels\"])\n",
        "slot_test = encode_token_labels(df_test[\"words\"], df_test[\"word_labels\"])\n",
        "\n",
        "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, intent_num_labels=None, slot_num_labels=None,\n",
        "                dropout_prob=0.1):\n",
        "        super().__init__(name=\"joint_intent_slot\")\n",
        "\n",
        "        self.bert = base_bert_model\n",
        "\n",
        "        # TODO: define the dropout, intent & slot classifier layers\n",
        "        ### YOUR CODE HERE ###\n",
        "        # Solution:\n",
        "        self.dropout = Dropout(dropout_prob)\n",
        "        self.intent_classifier = Dense(intent_num_labels,\n",
        "                                       name=\"intent_classifier\")\n",
        "        self.slot_classifier = Dense(slot_num_labels,\n",
        "                                     name=\"slot_classifier\")\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        tokens_output, pooled_output = self.bert(inputs, **kwargs, return_dict=False)\n",
        "\n",
        "        tokens_output = self.dropout(tokens_output,\n",
        "                                       training=kwargs.get(\"training\", False))\n",
        "        slot_logits = self.slot_classifier(tokens_output)\n",
        "        pooled_output = self.dropout(pooled_output,\n",
        "                                     training=kwargs.get(\"training\", False))\n",
        "        intent_logits = self.intent_classifier(pooled_output)\n",
        "\n",
        "        return slot_logits, intent_logits\n",
        "\n",
        "joint_model = JointIntentAndSlotFillingModel( \\\n",
        "    intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))\n",
        "\n",
        "losses = [SparseCategoricalCrossentropy(from_logits=True),\n",
        "          SparseCategoricalCrossentropy(from_logits=True)]\n",
        "\n",
        "joint_model.compile(optimizer=Adam(learning_rate=3e-5, epsilon=1e-08),\n",
        "                    loss=losses,\n",
        "                    metrics=[SparseCategoricalAccuracy('accuracy')], run_eagerly=True)\n",
        "\n",
        "history = joint_model.fit(encoded_train[\"input_ids\"], (slot_train, intent_train), \\\n",
        "    validation_data=(encoded_valid[\"input_ids\"], (slot_valid, intent_valid)), \\\n",
        "    epochs=1, batch_size=32)\n",
        "\n",
        "def show_predictions(text, intent_names, slot_names):\n",
        "    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n",
        "    outputs = joint_model(inputs)\n",
        "    slot_logits, intent_logits = outputs\n",
        "    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1]\n",
        "    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n",
        "    print(\"## Intent:\", intent_names[intent_id])\n",
        "    print(\"## Slots:\")\n",
        "    for token, slot_id in zip(tokenizer.tokenize(text), slot_ids):\n",
        "        print(f\"{token:>10} : {slot_names[slot_id]}\")\n",
        "\n",
        "def decode_predictions(text, intent_names, slot_names,\n",
        "                       intent_id, slot_ids):\n",
        "    info = {\"intent\": intent_names[intent_id]}\n",
        "    collected_slots = {}\n",
        "    active_slot_words = []\n",
        "    active_slot_name = None\n",
        "    for word in text.split():\n",
        "        tokens = tokenizer.tokenize(word)\n",
        "        current_word_slot_ids = slot_ids[:len(tokens)]\n",
        "        slot_ids = slot_ids[len(tokens):]\n",
        "        current_word_slot_name = slot_names[current_word_slot_ids[0]]\n",
        "        if current_word_slot_name == \"O\":\n",
        "            if active_slot_name:\n",
        "                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n",
        "                active_slot_words = []\n",
        "                active_slot_name = None\n",
        "        else:\n",
        "            # Naive BIO: handling: treat B- and I- the same...\n",
        "            new_slot_name = current_word_slot_name[2:]\n",
        "            if active_slot_name is None:\n",
        "                active_slot_words.append(word)\n",
        "                active_slot_name = new_slot_name\n",
        "            elif new_slot_name == active_slot_name:\n",
        "                active_slot_words.append(word)\n",
        "            else:\n",
        "                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n",
        "                active_slot_words = [word]\n",
        "                active_slot_name = new_slot_name\n",
        "    if active_slot_name:\n",
        "        collected_slots[active_slot_name] = \" \".join(active_slot_words)\n",
        "    info[\"slots\"] = collected_slots\n",
        "    return info\n",
        "\n",
        "def nlu(text, intent_names, slot_names):\n",
        "    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n",
        "    outputs = joint_model(inputs)\n",
        "    slot_logits, intent_logits = outputs\n",
        "    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1]\n",
        "    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n",
        "\n",
        "    return decode_predictions(text, intent_names, slot_names, intent_id, slot_ids)\n",
        "\n",
        "nlu(\"Book a table for two at Le Ritz for Friday night\", intent_names, slot_names)"
      ]
    }
  ]
}